{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_multi_img_with_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x7MQixkIMKM"
      },
      "source": [
        "# CNN을 통한 다중 이미지 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QQOaOx2ISqq"
      },
      "source": [
        "## Google Colab 링크\n",
        "\n",
        "https://colab.research.google.com/drive/1GcJCFBzVJS-zfe-3XAdXauxXtVHPsdmQ#scrollTo=7QQOaOx2ISqq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoatHnYKI8pK"
      },
      "source": [
        "**학습 데이터세트는 AI Hub에서 신청하여 받았고, 저작자는 한국지능정보사회진흥원이다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEmmauQdIX_g"
      },
      "source": [
        "## 참고자료"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tL3u_nwIclv"
      },
      "source": [
        "https://lsjsj92.tistory.com/387"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GknSALBMJB9e"
      },
      "source": [
        "# Google Colab 재시작\n",
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0xozfumIjB8"
      },
      "source": [
        "## 데이터세트 다운로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGtfA_LdIyde"
      },
      "source": [
        "구글 드라이브에 업로드한 zip 파일을 File ID로 다운받고 압축을 해제한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rCuKfnCIm7A",
        "outputId": "df3b1310-ea9a-4a4a-b956-82eb72f29933"
      },
      "source": [
        "!gdown --id 1AeNCnN8nOQI2q3p7kpeht2y6Aa95eHmT --output other_food.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AeNCnN8nOQI2q3p7kpeht2y6Aa95eHmT\n",
            "To: /content/other_food.zip\n",
            "348MB [00:02, 164MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2QPAQNnL6-b",
        "outputId": "fc35bfa7-ccbd-49e9-9fed-f2b516840fdb"
      },
      "source": [
        "!gdown --id 1WWP5_j1QXTKPYyREPuT7-d7IQdGgldgB --output img_test.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WWP5_j1QXTKPYyREPuT7-d7IQdGgldgB\n",
            "To: /content/img_test.zip\n",
            "\r  0% 0.00/1.90M [00:00<?, ?B/s]\r100% 1.90M/1.90M [00:00<00:00, 60.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quvn8e64MCL4"
      },
      "source": [
        "`other_food`는 훈련 데이터, `img_test`는 검증 데이터다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHD_iCiZL6CH"
      },
      "source": [
        "!unzip other_food.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TJppBKVIqqE"
      },
      "source": [
        "!unzip img_test.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC8WERctIpqu"
      },
      "source": [
        "경로를 지정하고 데이터세트 이미지 개수를 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ees9EGzDIy_w"
      },
      "source": [
        "import pathlib\n",
        "img_dir = 'other_food'\n",
        "data_dir = pathlib.Path(img_dir)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkEgbRQnI0mc",
        "outputId": "52dd9e79-9004-44c8-ecff-a598079651cb"
      },
      "source": [
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfJQFQ1zJVnc"
      },
      "source": [
        "## 멀티라벨 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBqWWHTYKwVF"
      },
      "source": [
        "데이터세트를 `콩자반`, `피자`, `후라이드치킨` 3가지로 분류한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwQwSuKDJtkp"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob, numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGkoiQUnJuKY",
        "outputId": "1ba73e6d-7de2-4b7d-83d3-3e65f49f974e"
      },
      "source": [
        "caltech_dir = \"other_food\"\n",
        "categories = ['beans', 'pizzas', 'fried_chickens']\n",
        "nb_classes = len(categories)\n",
        "\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for idx, food in enumerate(categories):\n",
        "    \n",
        "    #one-hot 돌리기.\n",
        "    label = [0 for i in range(nb_classes)]\n",
        "    label[idx] = 1\n",
        "\n",
        "    image_dir = caltech_dir + \"/\" + food\n",
        "    files = glob.glob(image_dir+\"/*.jpg\")\n",
        "    print(food, \" 파일 길이 : \", len(files))\n",
        "    for i, f in enumerate(files):\n",
        "        img = Image.open(f)\n",
        "        img = img.convert(\"RGB\")\n",
        "        img = img.resize((image_w, image_h))\n",
        "        data = np.asarray(img)\n",
        "\n",
        "        X.append(data)\n",
        "        y.append(label)\n",
        "\n",
        "        if i % 700 == 0:\n",
        "            print(food, \" : \", f)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beans  파일 길이 :  977\n",
            "beans  :  other_food/beans/Img_025_0457.jpg\n",
            "beans  :  other_food/beans/Img_025_0839.jpg\n",
            "pizzas  파일 길이 :  944\n",
            "pizzas  :  other_food/pizzas/Img_027_0341.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 4. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pizzas  :  other_food/pizzas/Img_027_0799.jpg\n",
            "fried_chickens  파일 길이 :  982\n",
            "fried_chickens  :  other_food/fried_chickens/Img_028_0861.jpg\n",
            "fried_chickens  :  other_food/fried_chickens/Img_028_0607.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d29CxCTbKIfy"
      },
      "source": [
        "!mkdir numpy_data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j4ILhO8KIIw",
        "outputId": "b15b4b1c-117c-4be4-f56d-129114d550e8"
      },
      "source": [
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "#1 0 0 0 이면 airplanes\n",
        "#0 1 0 0 이면 buddha 이런식\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "xy = (X_train, X_test, y_train, y_test)\n",
        "np.save(\"numpy_data/multi_image_data.npy\", xy)\n",
        "\n",
        "print(\"ok\", len(y))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ok 2903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgVY1w9MKXN-"
      },
      "source": [
        "## 모델 훈련"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hwG9mRDKYyM"
      },
      "source": [
        "이제 numpy 데이터를 불러와 훈련을 시작한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6MibNOGKfr-"
      },
      "source": [
        "import os, glob, numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nssWsJb2KggD",
        "outputId": "8ae72224-1991-4e68-e831-4489a13aceb6"
      },
      "source": [
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "\n",
        "X_train, X_test, y_train, y_test = np.load('numpy_data/multi_image_data.npy', allow_pickle=True)\n",
        "print(X_train.shape)\n",
        "print(X_train.shape[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2177, 64, 64, 3)\n",
            "2177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYBL7CwTK8vI"
      },
      "source": [
        "categories = ['beans', 'pizzas', 'fried_chickens']\n",
        "nb_classes = len(categories)\n",
        "\n",
        "#일반화\n",
        "X_train = X_train.astype(float) / 255\n",
        "X_test = X_test.astype(float) / 255"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZLjS5YCLEa4",
        "outputId": "6d404846-3ac6-4d97-ff16-edc167a60e9e"
      },
      "source": [
        "!mkdir model"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPq6cAkCK_uJ"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "    \n",
        "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "    \n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_dir = 'model'\n",
        "    \n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "    \n",
        "model_path = model_dir + '/multi_img_classification.model'\n",
        "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JopavN_PLKRO",
        "outputId": "afd1b091-1720-40ba-dce5-10fc5df88cf3"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 64, 64, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               4194560   \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 4,214,723\n",
            "Trainable params: 4,214,723\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyKNyc_WLM4f",
        "outputId": "9dbd0488-99bf-4a92-d752-7f6a2dfba1bf"
      },
      "source": [
        "#데이터셋이 적어서 validation을 그냥 test 데이터로 했습니다. \n",
        "#데이터셋이 충분하시면 이렇게 하시지 마시고 validation_split=0.2 이렇게 하셔서 테스트 셋으로 나누시길 권장합니다.\n",
        "# history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_split=0.2, callbacks=[checkpoint, early_stopping])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "55/55 [==============================] - 14s 241ms/step - loss: 1.0980 - accuracy: 0.3511 - val_loss: 1.0485 - val_accuracy: 0.6193\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.04849, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 2/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.9902 - accuracy: 0.5045 - val_loss: 0.7481 - val_accuracy: 0.6560\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.04849 to 0.74813, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 3/50\n",
            "55/55 [==============================] - 13s 235ms/step - loss: 0.7350 - accuracy: 0.6416 - val_loss: 0.6287 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.74813 to 0.62869, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 4/50\n",
            "55/55 [==============================] - 15s 266ms/step - loss: 0.6860 - accuracy: 0.6871 - val_loss: 0.5810 - val_accuracy: 0.7064\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.62869 to 0.58096, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 5/50\n",
            "55/55 [==============================] - 15s 262ms/step - loss: 0.6368 - accuracy: 0.6996 - val_loss: 0.5873 - val_accuracy: 0.7339\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.58096\n",
            "Epoch 6/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.5998 - accuracy: 0.7078 - val_loss: 0.5428 - val_accuracy: 0.7431\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.58096 to 0.54280, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 7/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.5692 - accuracy: 0.7306 - val_loss: 0.5433 - val_accuracy: 0.7339\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.54280\n",
            "Epoch 8/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.5799 - accuracy: 0.7157 - val_loss: 0.5186 - val_accuracy: 0.7546\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.54280 to 0.51858, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 9/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.5902 - accuracy: 0.7069 - val_loss: 0.5272 - val_accuracy: 0.7569\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.51858\n",
            "Epoch 10/50\n",
            "55/55 [==============================] - 13s 238ms/step - loss: 0.5676 - accuracy: 0.7390 - val_loss: 0.4926 - val_accuracy: 0.7821\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.51858 to 0.49261, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 11/50\n",
            "55/55 [==============================] - 13s 239ms/step - loss: 0.5149 - accuracy: 0.7633 - val_loss: 0.4895 - val_accuracy: 0.7959\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.49261 to 0.48951, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 12/50\n",
            "55/55 [==============================] - 13s 238ms/step - loss: 0.4906 - accuracy: 0.7802 - val_loss: 0.4694 - val_accuracy: 0.8073\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.48951 to 0.46941, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 13/50\n",
            "55/55 [==============================] - 13s 239ms/step - loss: 0.4519 - accuracy: 0.7887 - val_loss: 0.4955 - val_accuracy: 0.7936\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.46941\n",
            "Epoch 14/50\n",
            "55/55 [==============================] - 13s 238ms/step - loss: 0.4671 - accuracy: 0.7778 - val_loss: 0.4494 - val_accuracy: 0.8005\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.46941 to 0.44936, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 15/50\n",
            "55/55 [==============================] - 13s 238ms/step - loss: 0.4812 - accuracy: 0.7871 - val_loss: 0.4474 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.44936 to 0.44744, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 16/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.4381 - accuracy: 0.7918 - val_loss: 0.4326 - val_accuracy: 0.8257\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.44744 to 0.43258, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 17/50\n",
            "55/55 [==============================] - 13s 236ms/step - loss: 0.3931 - accuracy: 0.8088 - val_loss: 0.4298 - val_accuracy: 0.8257\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.43258 to 0.42978, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 18/50\n",
            "55/55 [==============================] - 13s 235ms/step - loss: 0.4053 - accuracy: 0.8081 - val_loss: 0.4203 - val_accuracy: 0.8303\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.42978 to 0.42034, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 19/50\n",
            "55/55 [==============================] - 13s 236ms/step - loss: 0.4029 - accuracy: 0.8079 - val_loss: 0.4162 - val_accuracy: 0.8394\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.42034 to 0.41619, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 20/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.3835 - accuracy: 0.8366 - val_loss: 0.5174 - val_accuracy: 0.8165\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.41619\n",
            "Epoch 21/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.4391 - accuracy: 0.8188 - val_loss: 0.3917 - val_accuracy: 0.8372\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.41619 to 0.39169, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 22/50\n",
            "55/55 [==============================] - 13s 239ms/step - loss: 0.3471 - accuracy: 0.8337 - val_loss: 0.3918 - val_accuracy: 0.8463\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.39169\n",
            "Epoch 23/50\n",
            "55/55 [==============================] - 13s 238ms/step - loss: 0.3345 - accuracy: 0.8558 - val_loss: 0.4442 - val_accuracy: 0.8234\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.39169\n",
            "Epoch 24/50\n",
            "55/55 [==============================] - 13s 239ms/step - loss: 0.4175 - accuracy: 0.8168 - val_loss: 0.3798 - val_accuracy: 0.8417\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.39169 to 0.37983, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 25/50\n",
            "55/55 [==============================] - 13s 239ms/step - loss: 0.3642 - accuracy: 0.8393 - val_loss: 0.3835 - val_accuracy: 0.8326\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.37983\n",
            "Epoch 26/50\n",
            "55/55 [==============================] - 13s 236ms/step - loss: 0.3218 - accuracy: 0.8578 - val_loss: 0.3921 - val_accuracy: 0.8417\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.37983\n",
            "Epoch 27/50\n",
            "55/55 [==============================] - 13s 235ms/step - loss: 0.3245 - accuracy: 0.8604 - val_loss: 0.3974 - val_accuracy: 0.8394\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.37983\n",
            "Epoch 28/50\n",
            "55/55 [==============================] - 13s 235ms/step - loss: 0.3052 - accuracy: 0.8776 - val_loss: 0.4069 - val_accuracy: 0.8372\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.37983\n",
            "Epoch 29/50\n",
            "55/55 [==============================] - 13s 236ms/step - loss: 0.3163 - accuracy: 0.8681 - val_loss: 0.4076 - val_accuracy: 0.8486\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.37983\n",
            "Epoch 30/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.3091 - accuracy: 0.8652 - val_loss: 0.3673 - val_accuracy: 0.8440\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.37983 to 0.36726, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 31/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.3346 - accuracy: 0.8534 - val_loss: 0.3686 - val_accuracy: 0.8486\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.36726\n",
            "Epoch 32/50\n",
            "55/55 [==============================] - 13s 236ms/step - loss: 0.2997 - accuracy: 0.8661 - val_loss: 0.3722 - val_accuracy: 0.8463\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.36726\n",
            "Epoch 33/50\n",
            "55/55 [==============================] - 13s 236ms/step - loss: 0.3004 - accuracy: 0.8854 - val_loss: 0.3629 - val_accuracy: 0.8532\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.36726 to 0.36292, saving model to model/multi_img_classification.model\n",
            "INFO:tensorflow:Assets written to: model/multi_img_classification.model/assets\n",
            "Epoch 34/50\n",
            "55/55 [==============================] - 13s 238ms/step - loss: 0.2883 - accuracy: 0.8781 - val_loss: 0.3877 - val_accuracy: 0.8509\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.36292\n",
            "Epoch 35/50\n",
            "55/55 [==============================] - 13s 236ms/step - loss: 0.2784 - accuracy: 0.8928 - val_loss: 0.3673 - val_accuracy: 0.8532\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.36292\n",
            "Epoch 36/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.2967 - accuracy: 0.8805 - val_loss: 0.3666 - val_accuracy: 0.8532\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.36292\n",
            "Epoch 37/50\n",
            "55/55 [==============================] - 13s 238ms/step - loss: 0.2806 - accuracy: 0.8824 - val_loss: 0.4075 - val_accuracy: 0.8234\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.36292\n",
            "Epoch 38/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.2861 - accuracy: 0.8789 - val_loss: 0.4120 - val_accuracy: 0.8440\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.36292\n",
            "Epoch 39/50\n",
            "55/55 [==============================] - 13s 237ms/step - loss: 0.2852 - accuracy: 0.8876 - val_loss: 0.3664 - val_accuracy: 0.8647\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.36292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg3PRGM-LR6d",
        "outputId": "f7d86e38-6ed8-4589-a33e-d9e4aea0c33f"
      },
      "source": [
        "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 1s 60ms/step - loss: 0.3751 - accuracy: 0.8650\n",
            "정확도 : 0.8650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Hc1x4uGyLT9C",
        "outputId": "44c4d64e-07e0-4e66-f4e5-edb5b7fa1cda"
      },
      "source": [
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "x_len = np.arange(len(y_loss))\n",
        "\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YpIs24ZK-36"
      },
      "source": [
        "## 모델 정확성 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eSgQKAHMZrF"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob, numpy as np\n",
        "from keras.models import load_model"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfi9HxyKMfzN",
        "outputId": "292786bc-b00d-4ac1-a9a3-9c08b53e1e2c"
      },
      "source": [
        "caltech_dir = \"img_test\"\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "X = []\n",
        "filenames = []\n",
        "files = glob.glob(caltech_dir+\"/*.*\")\n",
        "for i, f in enumerate(files):\n",
        "    img = Image.open(f)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((image_w, image_h))\n",
        "    data = np.asarray(img)\n",
        "    filenames.append(f)\n",
        "    X.append(data)\n",
        "\n",
        "X = np.array(X)\n",
        "model = load_model('model/multi_img_classification.model')\n",
        "\n",
        "prediction = model.predict(X)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "cnt = 0\n",
        "\n",
        "#이 비교는 그냥 파일들이 있으면 해당 파일과 비교. 카테고리와 함께 비교해서 진행하는 것은 _4 파일.\n",
        "for i in prediction:\n",
        "    pre_ans = i.argmax()  # 예측 레이블\n",
        "    print(i)\n",
        "    print(pre_ans)\n",
        "    pre_ans_str = ''\n",
        "    if pre_ans == 0: pre_ans_str = \"콩자반\"\n",
        "    elif pre_ans == 1: pre_ans_str = \"피자\"\n",
        "    else: pre_ans_str = \"후라이드치킨\"\n",
        "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"/\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
        "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    cnt += 1\n",
        "    # print(i.argmax()) #얘가 레이블 [1. 0. 0.] 이런식으로 되어 있는 것을 숫자로 바꿔주는 것.\n",
        "    # 즉 얘랑, 나중에 카테고리 데이터 불러와서 카테고리랑 비교를 해서 같으면 맞는거고, 아니면 틀린거로 취급하면 된다.\n",
        "    # 이걸 한 것은 _4.py에."
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.000 0.000 0.000]\n",
            "0\n",
            "해당 Img_027_0083.jpg이미지는 콩자반로 추정됩니다.\n",
            "[1.000 0.000 0.000]\n",
            "0\n",
            "해당 Img_027_0092.jpg이미지는 콩자반로 추정됩니다.\n",
            "[1.000 0.000 0.000]\n",
            "0\n",
            "해당 Img_028_0085.jpg이미지는 콩자반로 추정됩니다.\n",
            "[1.000 0.000 0.000]\n",
            "0\n",
            "해당 Img_027_0089.jpg이미지는 콩자반로 추정됩니다.\n",
            "[1.000 0.000 0.000]\n",
            "0\n",
            "해당 Img_025_0000.jpg이미지는 콩자반로 추정됩니다.\n",
            "[1.000 0.000 0.000]\n",
            "0\n",
            "해당 Img_028_0076.jpg이미지는 콩자반로 추정됩니다.\n",
            "[1.000 0.000 0.000]\n",
            "0\n",
            "해당 Img_025_0011.jpg이미지는 콩자반로 추정됩니다.\n",
            "[1.000 0.000 0.000]\n",
            "0\n",
            "해당 Img_028_0081.jpg이미지는 콩자반로 추정됩니다.\n",
            "[1.000 0.000 0.000]\n",
            "0\n",
            "해당 Img_025_0017.png이미지는 콩자반로 추정됩니다.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}